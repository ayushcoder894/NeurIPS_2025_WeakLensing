{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25771936",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BASELINE CNN MODEL - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"  Training samples: {X_train.shape[0]:,}\")\n",
    "print(f\"  Validation samples: {X_val.shape[0]:,}\")\n",
    "print(f\"  Test samples: {total_test_samples:,}\")\n",
    "print(f\"  Image dimensions: 1424 √ó 176 pixels\")\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Model Architecture:\")\n",
    "print(f\"  Type: Convolutional Neural Network (CNN)\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "\n",
    "print(f\"\\nüìà Training:\")\n",
    "print(f\"  Epochs trained: {len(history['train_loss'])}\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"  Loss function: KL Divergence\")\n",
    "print(f\"  Optimizer: Adam (lr={learning_rate})\")\n",
    "\n",
    "print(f\"\\nüéØ Validation Performance:\")\n",
    "print(f\"  Œ©_m RMSE: {np.sqrt(mean_squared_error(omega_m_true, omega_m_pred)):.6f}\")\n",
    "print(f\"  S_8 RMSE: {np.sqrt(mean_squared_error(s_8_true, s_8_pred)):.6f}\")\n",
    "print(f\"  Œ©_m R¬≤: {r2_score(omega_m_true, omega_m_pred):.4f}\")\n",
    "print(f\"  S_8 R¬≤: {r2_score(s_8_true, s_8_pred):.4f}\")\n",
    "\n",
    "print(f\"\\nüíæ Output Files:\")\n",
    "print(f\"  ‚úì Best model: best_model.pth\")\n",
    "print(f\"  ‚úì Submission (NumPy): submission_baseline_cnn.npy\")\n",
    "print(f\"  ‚úì Submission (CSV): submission_baseline_cnn.csv\")\n",
    "print(f\"  ‚úì Training history plot: training_history.png\")\n",
    "print(f\"  ‚úì Performance plot: model_performance.png\")\n",
    "print(f\"  ‚úì Test predictions plot: test_predictions.png\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps for Improvement:\")\n",
    "print(f\"  1. Data augmentation (rotations, flips, crops)\")\n",
    "print(f\"  2. Deeper network architecture (ResNet, EfficientNet)\")\n",
    "print(f\"  3. Ensemble multiple models\")\n",
    "print(f\"  4. Incorporate nuisance parameters in training\")\n",
    "print(f\"  5. Use attention mechanisms or Vision Transformers\")\n",
    "print(f\"  6. Experiment with different loss functions\")\n",
    "print(f\"  7. Add power spectrum features as additional input\")\n",
    "print(f\"  8. Apply test-time augmentation\")\n",
    "print(f\"  9. Hyperparameter tuning (learning rate, architecture)\")\n",
    "print(f\"  10. Cross-validation for more robust evaluation\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Baseline model training and evaluation complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37abbf47",
   "metadata": {},
   "source": [
    "## 14. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f32f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Omega_m distribution\n",
    "axes[0, 0].hist(test_omega_m, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Predicted Œ©_m')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Predicted Œ©_m')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# S_8 distribution\n",
    "axes[0, 1].hist(test_s_8, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[0, 1].set_xlabel('Predicted S_8')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Predicted S_8')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Joint distribution\n",
    "axes[0, 2].scatter(test_omega_m, test_s_8, alpha=0.5, s=20, c=range(len(test_omega_m)), cmap='viridis')\n",
    "axes[0, 2].set_xlabel('Predicted Œ©_m')\n",
    "axes[0, 2].set_ylabel('Predicted S_8')\n",
    "axes[0, 2].set_title('Joint Distribution of Predictions')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Omega_m uncertainty distribution\n",
    "axes[1, 0].hist(test_sigma_omega_m, bins=50, alpha=0.7, edgecolor='black', color='green')\n",
    "axes[1, 0].set_xlabel('Predicted œÉ_Œ©_m')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Œ©_m Uncertainties')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# S_8 uncertainty distribution\n",
    "axes[1, 1].hist(test_sigma_s_8, bins=50, alpha=0.7, edgecolor='black', color='red')\n",
    "axes[1, 1].set_xlabel('Predicted œÉ_S_8')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of S_8 Uncertainties')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Uncertainty vs prediction\n",
    "axes[1, 2].scatter(test_omega_m, test_sigma_omega_m, alpha=0.4, s=15, label='Œ©_m')\n",
    "axes[1, 2].scatter(test_s_8, test_sigma_s_8, alpha=0.4, s=15, label='S_8', color='orange')\n",
    "axes[1, 2].set_xlabel('Predicted Value')\n",
    "axes[1, 2].set_ylabel('Predicted Uncertainty (œÉ)')\n",
    "axes[1, 2].set_title('Predictions vs Uncertainties')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Test Set Predictions', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'test_predictions.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Test prediction plots saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352873e",
   "metadata": {},
   "source": [
    "## 13. Visualize Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f830be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission array\n",
    "# Format: [Omega_m, S_8, sigma_Omega_m, sigma_S_8] for each test sample\n",
    "submission = np.column_stack([test_omega_m, test_s_8, test_sigma_omega_m, test_sigma_s_8])\n",
    "\n",
    "print(f\"Submission shape: {submission.shape}\")\n",
    "print(f\"Submission columns: [Omega_m, S_8, sigma_Omega_m, sigma_S_8]\")\n",
    "\n",
    "# Save submission\n",
    "submission_file = os.path.join(data_dir, 'submission_baseline_cnn.npy')\n",
    "np.save(submission_file, submission)\n",
    "print(f\"\\nSubmission saved to: {submission_file}\")\n",
    "\n",
    "# Also save as CSV for easy viewing\n",
    "import pandas as pd\n",
    "submission_df = pd.DataFrame(submission, columns=['Omega_m', 'S_8', 'sigma_Omega_m', 'sigma_S_8'])\n",
    "submission_csv = os.path.join(data_dir, 'submission_baseline_cnn.csv')\n",
    "submission_df.to_csv(submission_csv, index=False)\n",
    "print(f\"Submission CSV saved to: {submission_csv}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b828e8",
   "metadata": {},
   "source": [
    "## 12. Save Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22688d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for test set\n",
    "model.eval()\n",
    "test_predictions = []\n",
    "test_uncertainties = []\n",
    "\n",
    "print(\"Generating test set predictions...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (data, _) in enumerate(test_loader):\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        \n",
    "        predictions = output[:, :2].cpu().numpy()  # [Omega_m, S_8]\n",
    "        log_vars = output[:, 2:].cpu().numpy()  # [log_var_Omega_m, log_var_S_8]\n",
    "        uncertainties = np.sqrt(np.exp(log_vars))  # Convert to std dev\n",
    "        \n",
    "        test_predictions.append(predictions)\n",
    "        test_uncertainties.append(uncertainties)\n",
    "        \n",
    "        if (batch_idx + 1) % 50 == 0:\n",
    "            print(f\"  Processed {(batch_idx + 1) * batch_size}/{total_test_samples} samples\")\n",
    "\n",
    "test_predictions = np.vstack(test_predictions)\n",
    "test_uncertainties = np.vstack(test_uncertainties)\n",
    "\n",
    "print(f\"\\nTest predictions shape: {test_predictions.shape}\")\n",
    "print(f\"Test uncertainties shape: {test_uncertainties.shape}\")\n",
    "\n",
    "# Extract parameters\n",
    "test_omega_m = test_predictions[:, 0]\n",
    "test_s_8 = test_predictions[:, 1]\n",
    "test_sigma_omega_m = test_uncertainties[:, 0]\n",
    "test_sigma_s_8 = test_uncertainties[:, 1]\n",
    "\n",
    "print(\"\\nTest Set Predictions Summary:\")\n",
    "print(f\"Œ©_m: mean={test_omega_m.mean():.4f}, std={test_omega_m.std():.4f}, range=[{test_omega_m.min():.4f}, {test_omega_m.max():.4f}]\")\n",
    "print(f\"S_8: mean={test_s_8.mean():.4f}, std={test_s_8.std():.4f}, range=[{test_s_8.min():.4f}, {test_s_8.max():.4f}]\")\n",
    "print(f\"œÉ_Œ©_m: mean={test_sigma_omega_m.mean():.6f}, median={np.median(test_sigma_omega_m):.6f}\")\n",
    "print(f\"œÉ_S_8: mean={test_sigma_s_8.mean():.6f}, median={np.median(test_sigma_s_8):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec50b575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "print(f\"Test data shape: {kappa_test.shape}\")\n",
    "\n",
    "# Reshape test data similar to training data\n",
    "if len(kappa_test.shape) == 3:\n",
    "    n_test_cosmo, n_test_samples, n_pixels = kappa_test.shape\n",
    "    total_test_samples = n_test_cosmo * n_test_samples\n",
    "    X_test_flat = kappa_test.reshape(total_test_samples, -1)\n",
    "    print(f\"Total test samples: {total_test_samples}\")\n",
    "else:\n",
    "    X_test_flat = kappa_test\n",
    "    total_test_samples = X_test_flat.shape[0]\n",
    "    print(f\"Total test samples: {total_test_samples}\")\n",
    "\n",
    "print(f\"X_test_flat shape: {X_test_flat.shape}\")\n",
    "\n",
    "# Reconstruct 2D images for test set\n",
    "print(f\"Reconstructing {total_test_samples} test images...\")\n",
    "X_test_2d = np.zeros((total_test_samples, 1424, 176), dtype=np.float32)\n",
    "for i in range(total_test_samples):\n",
    "    X_test_2d[i] = reconstruct_image(X_test_flat[i], mask)\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(f\"  Processed {i + 1}/{total_test_samples} images\")\n",
    "\n",
    "print(f\"Test data 2D shape: {X_test_2d.shape}\")\n",
    "\n",
    "# Create test dataset and loader\n",
    "test_dataset = ConvergenceMapDataset(X_test_2d, np.zeros((total_test_samples, 2)))  # Dummy targets\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa98920b",
   "metadata": {},
   "source": [
    "## 11. Generate Predictions for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822e5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Omega_m: True vs Predicted\n",
    "axes[0, 0].scatter(omega_m_true, omega_m_pred, alpha=0.5, s=20)\n",
    "axes[0, 0].plot([omega_m_true.min(), omega_m_true.max()], \n",
    "                [omega_m_true.min(), omega_m_true.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('True Œ©_m')\n",
    "axes[0, 0].set_ylabel('Predicted Œ©_m')\n",
    "axes[0, 0].set_title('Œ©_m: True vs Predicted')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# S_8: True vs Predicted\n",
    "axes[0, 1].scatter(s_8_true, s_8_pred, alpha=0.5, s=20, color='orange')\n",
    "axes[0, 1].plot([s_8_true.min(), s_8_true.max()], \n",
    "                [s_8_true.min(), s_8_true.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('True S_8')\n",
    "axes[0, 1].set_ylabel('Predicted S_8')\n",
    "axes[0, 1].set_title('S_8: True vs Predicted')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Joint distribution\n",
    "axes[0, 2].scatter(omega_m_true, s_8_true, alpha=0.3, s=20, label='True', color='blue')\n",
    "axes[0, 2].scatter(omega_m_pred, s_8_pred, alpha=0.3, s=20, label='Predicted', color='red')\n",
    "axes[0, 2].set_xlabel('Œ©_m')\n",
    "axes[0, 2].set_ylabel('S_8')\n",
    "axes[0, 2].set_title('Joint Distribution')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Omega_m: Error distribution\n",
    "axes[1, 0].hist(error_omega_m, bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].axvline(0, color='red', linestyle='--', lw=2)\n",
    "axes[1, 0].set_xlabel('Error (Predicted - True)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title(f'Œ©_m Error Distribution\\\\nMean: {error_omega_m.mean():.6f}, Std: {error_omega_m.std():.6f}')\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# S_8: Error distribution\n",
    "axes[1, 1].hist(error_s_8, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[1, 1].axvline(0, color='red', linestyle='--', lw=2)\n",
    "axes[1, 1].set_xlabel('Error (Predicted - True)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title(f'S_8 Error Distribution\\\\nMean: {error_s_8.mean():.6f}, Std: {error_s_8.std():.6f}')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Uncertainty calibration\n",
    "axes[1, 2].scatter(sigma_omega_m, np.abs(error_omega_m), alpha=0.5, s=20, label='Œ©_m')\n",
    "axes[1, 2].scatter(sigma_s_8, np.abs(error_s_8), alpha=0.5, s=20, label='S_8', color='orange')\n",
    "axes[1, 2].plot([0, max(sigma_omega_m.max(), sigma_s_8.max())], \n",
    "                [0, max(sigma_omega_m.max(), sigma_s_8.max())], 'r--', lw=2, label='Perfect calibration')\n",
    "axes[1, 2].set_xlabel('Predicted Uncertainty (œÉ)')\n",
    "axes[1, 2].set_ylabel('Absolute Error')\n",
    "axes[1, 2].set_title('Uncertainty Calibration')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Performance on Validation Set', fontsize=14, y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'model_performance.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Performance plots saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5726a97a",
   "metadata": {},
   "source": [
    "## 10. Visualize Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217de9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "omega_m_pred = all_predictions[:, 0]\n",
    "s_8_pred = all_predictions[:, 1]\n",
    "omega_m_true = all_targets[:, 0]\n",
    "s_8_true = all_targets[:, 1]\n",
    "sigma_omega_m = all_uncertainties[:, 0]\n",
    "sigma_s_8 = all_uncertainties[:, 1]\n",
    "\n",
    "# Compute errors\n",
    "error_omega_m = omega_m_pred - omega_m_true\n",
    "error_s_8 = s_8_pred - s_8_true\n",
    "\n",
    "# Compute metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"VALIDATION SET PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nŒ©_m (Matter Density Fraction):\")\n",
    "print(f\"  MSE:  {mean_squared_error(omega_m_true, omega_m_pred):.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(omega_m_true, omega_m_pred)):.6f}\")\n",
    "print(f\"  MAE:  {mean_absolute_error(omega_m_true, omega_m_pred):.6f}\")\n",
    "print(f\"  R¬≤:   {r2_score(omega_m_true, omega_m_pred):.6f}\")\n",
    "print(f\"  Mean predicted uncertainty: {sigma_omega_m.mean():.6f}\")\n",
    "print(f\"  Median predicted uncertainty: {np.median(sigma_omega_m):.6f}\")\n",
    "\n",
    "print(\"\\nS_8 (Matter Fluctuation Amplitude):\")\n",
    "print(f\"  MSE:  {mean_squared_error(s_8_true, s_8_pred):.6f}\")\n",
    "print(f\"  RMSE: {np.sqrt(mean_squared_error(s_8_true, s_8_pred)):.6f}\")\n",
    "print(f\"  MAE:  {mean_absolute_error(s_8_true, s_8_pred):.6f}\")\n",
    "print(f\"  R¬≤:   {r2_score(s_8_true, s_8_pred):.6f}\")\n",
    "print(f\"  Mean predicted uncertainty: {sigma_s_8.mean():.6f}\")\n",
    "print(f\"  Median predicted uncertainty: {np.median(sigma_s_8):.6f}\")\n",
    "\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84ba72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "checkpoint = torch.load(os.path.join(data_dir, 'best_model.pth'))\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "\n",
    "# Get predictions on validation set\n",
    "model.eval()\n",
    "all_predictions = []\n",
    "all_targets = []\n",
    "all_uncertainties = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in val_loader:\n",
    "        data = data.to(device)\n",
    "        output = model(data)\n",
    "        \n",
    "        predictions = output[:, :2].cpu().numpy()  # [Omega_m, S_8]\n",
    "        log_vars = output[:, 2:].cpu().numpy()  # [log_var_Omega_m, log_var_S_8]\n",
    "        uncertainties = np.sqrt(np.exp(log_vars))  # Convert to std dev\n",
    "        \n",
    "        all_predictions.append(predictions)\n",
    "        all_targets.append(target.numpy())\n",
    "        all_uncertainties.append(uncertainties)\n",
    "\n",
    "all_predictions = np.vstack(all_predictions)\n",
    "all_targets = np.vstack(all_targets)\n",
    "all_uncertainties = np.vstack(all_uncertainties)\n",
    "\n",
    "print(f\"Predictions shape: {all_predictions.shape}\")\n",
    "print(f\"Targets shape: {all_targets.shape}\")\n",
    "print(f\"Uncertainties shape: {all_uncertainties.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036980e",
   "metadata": {},
   "source": [
    "## 9. Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf0f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot KL loss\n",
    "axes[0].plot(history['train_loss'], label='Train', marker='o')\n",
    "axes[0].plot(history['val_loss'], label='Validation', marker='s')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('KL Divergence Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot MSE\n",
    "axes[1].plot(history['train_mse'], label='Train', marker='o')\n",
    "axes[1].plot(history['val_mse'], label='Validation', marker='s')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Mean Squared Error')\n",
    "axes[1].set_title('Training and Validation MSE')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(data_dir, 'training_history.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Training history plots saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54da11a3",
   "metadata": {},
   "source": [
    "## 8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d4bf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_mse = train_epoch(model, train_loader, optimizer, device)\n",
    "    val_loss, val_mse = validate(model, val_loader, device)\n",
    "    \n",
    "    # Update scheduler\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Save history\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_mse'].append(train_mse)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_mse'].append(val_mse)\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch+1:02d}/{num_epochs} | \"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train MSE: {train_mse:.6f} | \"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val MSE: {val_mse:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, os.path.join(data_dir, 'best_model.pth'))\n",
    "        print(f\"  ‚Üí Best model saved! (Val Loss: {val_loss:.4f})\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered after {epoch+1} epochs!\")\n",
    "        break\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"Training completed!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bd7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, device):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_mse = 0\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        # Compute losses\n",
    "        loss = kl_divergence_loss(output, target)\n",
    "        mse = mse_loss(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_mse += mse.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_mse = total_mse / len(train_loader)\n",
    "    \n",
    "    return avg_loss, avg_mse\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    \"\"\"Validate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_mse = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            loss = kl_divergence_loss(output, target)\n",
    "            mse = mse_loss(output, target)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_mse += mse.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    avg_mse = total_mse / len(val_loader)\n",
    "    \n",
    "    return avg_loss, avg_mse\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "learning_rate = 1e-3\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# Training configuration\n",
    "num_epochs = 20\n",
    "best_val_loss = float('inf')\n",
    "patience = 7\n",
    "patience_counter = 0\n",
    "\n",
    "# History\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_mse': [],\n",
    "    'val_loss': [],\n",
    "    'val_mse': []\n",
    "}\n",
    "\n",
    "print(f\"Starting training for {num_epochs} epochs...\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Optimizer: Adam\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9346e8",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0c4156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_divergence_loss(predictions, targets):\n",
    "    \"\"\"\n",
    "    KL Divergence loss for uncertainty estimation.\n",
    "    \n",
    "    Args:\n",
    "        predictions: Tensor of shape (batch_size, 4) \n",
    "                    [Omega_m_pred, S_8_pred, log_var_Omega_m, log_var_S_8]\n",
    "        targets: Tensor of shape (batch_size, 2) [Omega_m_true, S_8_true]\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar tensor\n",
    "    \"\"\"\n",
    "    # Split predictions\n",
    "    omega_m_pred = predictions[:, 0]\n",
    "    s_8_pred = predictions[:, 1]\n",
    "    log_var_omega_m = predictions[:, 2]\n",
    "    log_var_s_8 = predictions[:, 3]\n",
    "    \n",
    "    # Split targets\n",
    "    omega_m_true = targets[:, 0]\n",
    "    s_8_true = targets[:, 1]\n",
    "    \n",
    "    # Compute variance from log variance (for numerical stability)\n",
    "    var_omega_m = torch.exp(log_var_omega_m)\n",
    "    var_s_8 = torch.exp(log_var_s_8)\n",
    "    \n",
    "    # KL divergence loss components\n",
    "    # Loss = (pred - true)^2 / var + log(var)\n",
    "    loss_omega_m = ((omega_m_pred - omega_m_true) ** 2) / var_omega_m + log_var_omega_m\n",
    "    loss_s_8 = ((s_8_pred - s_8_true) ** 2) / var_s_8 + log_var_s_8\n",
    "    \n",
    "    # Total loss (mean over batch)\n",
    "    total_loss = torch.mean(loss_omega_m + loss_s_8)\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def mse_loss(predictions, targets):\n",
    "    \"\"\"Simple MSE loss for comparison\"\"\"\n",
    "    omega_m_pred = predictions[:, 0]\n",
    "    s_8_pred = predictions[:, 1]\n",
    "    omega_m_true = targets[:, 0]\n",
    "    s_8_true = targets[:, 1]\n",
    "    \n",
    "    loss = torch.mean((omega_m_pred - omega_m_true) ** 2 + (s_8_pred - s_8_true) ** 2)\n",
    "    return loss\n",
    "\n",
    "# Test the loss function\n",
    "test_predictions = torch.randn(32, 4)\n",
    "test_targets = torch.randn(32, 2)\n",
    "test_loss = kl_divergence_loss(test_predictions, test_targets)\n",
    "print(f\"Test KL loss: {test_loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6551cf5f",
   "metadata": {},
   "source": [
    "## 6. Define KL Divergence Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817b1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeakLensingCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN for predicting cosmological parameters from weak lensing convergence maps.\n",
    "    Outputs: [Omega_m, S_8, log_var_Omega_m, log_var_S_8]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_channels=1):\n",
    "        super(WeakLensingCNN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=5, stride=2, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool2d((4, 4))  # Fixed output size\n",
    "        )\n",
    "        \n",
    "        # Calculate flattened size\n",
    "        self.flat_features = 256 * 4 * 4\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.flat_features, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 4)  # Output: [Omega_m, S_8, log_var_Omega_m, log_var_S_8]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = WeakLensingCNN(input_channels=1).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model: WeakLensingCNN\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8435ff69",
   "metadata": {},
   "source": [
    "## 5. Define CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf934f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvergenceMapDataset(Dataset):\n",
    "    \"\"\"Dataset for weak lensing convergence maps\"\"\"\n",
    "    \n",
    "    def __init__(self, images, targets, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: numpy array of shape (N, H, W)\n",
    "            targets: numpy array of shape (N, 2) - [Omega_m, S_8]\n",
    "            transform: optional transform to apply\n",
    "        \"\"\"\n",
    "        self.images = torch.FloatTensor(images).unsqueeze(1)  # Add channel dim: (N, 1, H, W)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.images[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, target\n",
    "\n",
    "# Split data into train and validation sets\n",
    "X_train, X_val, y_train_split, y_val = train_test_split(\n",
    "    X_train_2d, y_train, test_size=0.15, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Validation set: {X_val.shape[0]} samples\")\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = ConvergenceMapDataset(X_train, y_train_split)\n",
    "val_dataset = ConvergenceMapDataset(X_val, y_val)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"\\nBatch size: {batch_size}\")\n",
    "print(f\"Training batches: {len(train_loader)}\")\n",
    "print(f\"Validation batches: {len(val_loader)}\")\n",
    "\n",
    "# Test the data loader\n",
    "sample_batch, sample_targets = next(iter(train_loader))\n",
    "print(f\"\\nSample batch shape: {sample_batch.shape}\")  # (batch_size, 1, 1424, 176)\n",
    "print(f\"Sample targets shape: {sample_targets.shape}\")  # (batch_size, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ec1442",
   "metadata": {},
   "source": [
    "## 4. Create PyTorch Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057bc559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is currently flattened. We need to reconstruct the 2D images using the mask\n",
    "# Mask shape is (1424, 176), and flattened valid pixels = 132019\n",
    "\n",
    "# Find valid pixel positions\n",
    "valid_mask = (mask > 0).flatten()\n",
    "n_valid_pixels = valid_mask.sum()\n",
    "print(f\"Number of valid pixels in mask: {n_valid_pixels}\")\n",
    "print(f\"Expected from data: {X_train_flat.shape[1]}\")\n",
    "\n",
    "# Create function to reconstruct 2D images from flattened data\n",
    "def reconstruct_image(flat_data, mask):\n",
    "    \"\"\"\n",
    "    Reconstruct 2D image from flattened valid pixels\n",
    "    flat_data: (n_valid_pixels,)\n",
    "    mask: (H, W) boolean mask\n",
    "    returns: (H, W) image\n",
    "    \"\"\"\n",
    "    H, W = mask.shape\n",
    "    image = np.zeros((H, W), dtype=flat_data.dtype)\n",
    "    image[mask > 0] = flat_data\n",
    "    return image\n",
    "\n",
    "# Reconstruct a sample image to verify\n",
    "sample_idx = 0\n",
    "sample_2d = reconstruct_image(X_train_flat[sample_idx], mask)\n",
    "print(f\"\\nReconstructed image shape: {sample_2d.shape}\")\n",
    "print(f\"Reconstructed image stats: min={sample_2d.min():.4f}, max={sample_2d.max():.4f}, mean={sample_2d.mean():.4f}\")\n",
    "\n",
    "# Visualize sample\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(sample_2d, cmap='RdBu_r', aspect='auto')\n",
    "plt.colorbar(label='Œ∫ (convergence)')\n",
    "plt.title(f'Sample Convergence Map\\\\nŒ©_m={y_omega_m[sample_idx]:.4f}, S_8={y_s_8[sample_idx]:.4f}')\n",
    "plt.xlabel('Width (pixels)')\n",
    "plt.ylabel('Height (pixels)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Reconstruct all training images\n",
    "print(f\"\\nReconstructing all {total_samples} training images...\")\n",
    "X_train_2d = np.zeros((total_samples, 1424, 176), dtype=np.float32)\n",
    "for i in range(total_samples):\n",
    "    X_train_2d[i] = reconstruct_image(X_train_flat[i], mask)\n",
    "    if (i + 1) % 5000 == 0:\n",
    "        print(f\"  Processed {i + 1}/{total_samples} images\")\n",
    "\n",
    "print(f\"Final training data shape: {X_train_2d.shape}\")\n",
    "print(f\"Memory usage: {X_train_2d.nbytes / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1429eb",
   "metadata": {},
   "source": [
    "## 3. Reshape Data to 2D Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451b03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_dir = r'c:\\ML\\Challenges\\NeurIPS_2025'\n",
    "label_file = os.path.join(data_dir, 'label.npy')\n",
    "kappa_file = os.path.join(data_dir, 'WIDE12H_bin2_2arcmin_kappa.npy')\n",
    "kappa_test_file = os.path.join(data_dir, 'WIDE12H_bin2_2arcmin_kappa_noisy_test.npy')\n",
    "mask_file = os.path.join(data_dir, 'WIDE12H_bin2_2arcmin_mask.npy')\n",
    "\n",
    "print(\"Loading data...\")\n",
    "# Load data\n",
    "labels = np.load(label_file)  # Shape: (101, 256, 5)\n",
    "kappa_train = np.load(kappa_file)  # Shape: (101, 256, 132019)\n",
    "kappa_test = np.load(kappa_test_file)  # Shape: (?, ?, 132019)\n",
    "mask = np.load(mask_file)  # Shape: (1424, 176)\n",
    "\n",
    "print(f\"Labels shape: {labels.shape}\")\n",
    "print(f\"Training kappa shape: {kappa_train.shape}\")\n",
    "print(f\"Test kappa shape: {kappa_test.shape}\")\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "\n",
    "# Extract cosmological parameters (Omega_m and S_8)\n",
    "# Labels structure: [:, :, 0] = Omega_m, [:, :, 1] = S_8, [:, :, 2:] = nuisance params\n",
    "omega_m = labels[:, :, 0]  # Shape: (101, 256)\n",
    "s_8 = labels[:, :, 1]  # Shape: (101, 256)\n",
    "\n",
    "# Reshape data for training\n",
    "# Flatten cosmology and sample dimensions: (101, 256) -> (101*256,)\n",
    "n_cosmologies, n_samples_per_cosmo = omega_m.shape\n",
    "total_samples = n_cosmologies * n_samples_per_cosmo\n",
    "\n",
    "X_train_flat = kappa_train.reshape(total_samples, -1)  # (25856, 132019)\n",
    "y_omega_m = omega_m.flatten()  # (25856,)\n",
    "y_s_8 = s_8.flatten()  # (25856,)\n",
    "\n",
    "print(f\"\\nTotal training samples: {total_samples}\")\n",
    "print(f\"X_train shape: {X_train_flat.shape}\")\n",
    "print(f\"Y Omega_m shape: {y_omega_m.shape}\")\n",
    "print(f\"Y S_8 shape: {y_s_8.shape}\")\n",
    "\n",
    "# Combine target parameters\n",
    "y_train = np.column_stack([y_omega_m, y_s_8])  # (25856, 2)\n",
    "print(f\"Y_train (combined) shape: {y_train.shape}\")\n",
    "\n",
    "print(\"\\nTarget statistics:\")\n",
    "print(f\"Omega_m: mean={y_omega_m.mean():.4f}, std={y_omega_m.std():.4f}, range=[{y_omega_m.min():.4f}, {y_omega_m.max():.4f}]\")\n",
    "print(f\"S_8: mean={y_s_8.mean():.4f}, std={y_s_8.std():.4f}, range=[{y_s_8.min():.4f}, {y_s_8.max():.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83160628",
   "metadata": {},
   "source": [
    "## 2. Load and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da84c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d224ac9",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1092f35f",
   "metadata": {},
   "source": [
    "# NeurIPS 2025 Weak Lensing Challenge - Baseline CNN Model\n",
    "\n",
    "This notebook implements a baseline Convolutional Neural Network (CNN) for predicting cosmological parameters (Œ©_m, S_8) and their uncertainties from weak lensing convergence maps.\n",
    "\n",
    "## Approach: CNN Direct Prediction with KL Divergence Loss\n",
    "\n",
    "The model predicts:\n",
    "- **Point estimates**: (Œ©ÃÇ_m, ≈ú_8)\n",
    "- **Uncertainties**: (œÉÃÇ_Œ©_m, œÉÃÇ_S_8)\n",
    "\n",
    "Training uses KL divergence loss to optimize both predictions and uncertainties simultaneously."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
